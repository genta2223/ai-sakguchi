import logging
import os
import time
import threading
import json
import numpy as np
from queue import Queue, Empty

import re
import streamlit as st
from brain import generate_response
from tts import synthesize_speech
from core_paths import LOCAL_STATIC_DIR
from langchain_google_genai import GoogleGenerativeAIEmbeddings

def normalize_text(text: str) -> str:
    """æ–‡å­—åˆ—ã®æ­£è¦åŒ–ï¼šä¸è¦ãªè¨˜å·ã‚„ç©ºç™½ã‚’å‰Šé™¤ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æŸ”è»Ÿãªç…§åˆç”¨ï¼‰"""
    if not text: return ""
    # å…¨è§’åŠè§’ç©ºç™½ã€æ”¹è¡Œã€æ„Ÿå˜†ç¬¦ãªã©ã‚’å…¨ã¦é™¤å»
    return re.sub(r'[â€¦\.\?\!ã€‚ï¼Ÿï¼\s\n\rã€€]+', '', text).strip()

# ============================================================
# Configuration & Worker
# ============================================================
logger = logging.getLogger(__name__)

# --- Cloud Concurrency Limits ---
IS_CLOUD = os.getenv("STREAMLIT_SERVER_BASE_URL", "") != ""
MAX_CONCURRENCY = 10 if IS_CLOUD else 3
SEMAPHORE = threading.Semaphore(MAX_CONCURRENCY)

FAQ_CACHE = []
FAQ_EMBEDDINGS = None
EMBEDDER = None

def init_faq_cache(api_key: str):
    global FAQ_CACHE, FAQ_EMBEDDINGS, EMBEDDER
    if FAQ_CACHE: return
    
    FAQ_CACHE = []
    
    # 1. ç¬¬1å±¤ï¼ˆè–åŸŸï¼‰ãƒã‚¹ã‚¿ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿è¾¼ã¿ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
    cache_file = LOCAL_STATIC_DIR / "faq_cache.json"
    if cache_file.exists():
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                master_cache = json.load(f)
                for item in master_cache:
                    item["source"] = "master" # ãƒãƒ¼ã‚¯ã¥ã‘
                FAQ_CACHE.extend(master_cache)
        except Exception as e:
            logger.error(f"[Worker] Failed to load master cache: {e}")

    # 2. ç¬¬2å±¤ï¼ˆé‡è‰¯è³ªå•æ‹¡å¼µï¼‰ã‚¨ã‚­ã‚¹ãƒˆãƒ©ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿è¾¼ã¿
    extra_cache_file = LOCAL_STATIC_DIR / "extra_cache.json"
    if extra_cache_file.exists():
        try:
            with open(extra_cache_file, "r", encoding="utf-8") as f:
                extra_cache = json.load(f)
                for item in extra_cache:
                    item["source"] = "extra"
                FAQ_CACHE.extend(extra_cache)
            logger.info(f"[Worker] Loaded {len(extra_cache)} extra FAQs.")
        except Exception as e:
            logger.error(f"[Worker] Failed to load extra cache: {e}")
            
    try:
        # ç…§åˆç”¨ã‚­ãƒ¼ã‚’äº‹å‰ã«æº–å‚™
        for c_item in FAQ_CACHE:
            if "question" in c_item:
                c_item["norm_key"] = normalize_text(c_item["question"])
                
        # ğŸš€ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆé‡ã„å‡¦ç†ï¼‰ã¯ã“ã“ã§ã¯è¡Œã‚ãšã€_worker_loopå†…ã§Lazy Loadï¼ˆé…å»¶å®Ÿè¡Œï¼‰ã™ã‚‹ã‚ˆã†å¤‰æ›´
        logger.info(f"[Worker] Loaded total {len(FAQ_CACHE)} FAQs. Embeddings will be lazy-loaded on first miss.")
    except Exception as e:
        logger.error(f"[Worker] Failed to init FAQ cache normalization: {e}")

def _worker_loop(input_queue: Queue, output_queue: Queue, stop_event: threading.Event, 
                 google_api_key: str, creds_json: str, private_key: str, client_email: str):
    """Background thread: Process Gemini and TTS with explicitly injected secrets."""
    global FAQ_CACHE, FAQ_EMBEDDINGS, EMBEDDER
    logger.info("[Worker] Thread started with injected secrets (Bucket Relay).")
    # ğŸŒŸ Silent Pre-load: èµ·å‹•ç›´å¾Œã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§é‡ã„å‡¦ç†ã‚’é™ã‹ã«å®Œäº†ã•ã›ã‚‹
    init_faq_cache(google_api_key)
    if FAQ_CACHE and EMBEDDER is None:
        try:
            EMBEDDER = GoogleGenerativeAIEmbeddings(
                model="models/gemini-embedding-001",
                google_api_key=google_api_key
            )
            questions = [item.get("question", "") for item in FAQ_CACHE if item.get("question")]
            if questions:
                embeddings = EMBEDDER.embed_documents(questions)
                FAQ_EMBEDDINGS = np.array(embeddings)
            else:
                FAQ_EMBEDDINGS = np.array([])
        except Exception:
            pass # Silent fail to avoid polluting WebSocket with logs
    while not stop_event.is_set():
        try:
            item = input_queue.get(timeout=1)
        except Empty:
            continue

        # Use Semaphore to limit simultaneous AI/TTS generation
        with SEMAPHORE:
            try:
                best_match_item = None
                is_system = getattr(item, "source", None) == "system"
                is_greeting = getattr(item, "is_initial_greeting", False)
                
                cache_to_repair = None
                
                if FAQ_CACHE and not is_system and not is_greeting:
                    try:
                        output_queue.put({"type": "debug", "msg": "ğŸ” æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢ä¸­..."})
                        norm_query = normalize_text(item.message_text)
                        best_idx = -1
                        max_sim = 0.0
                        
                        # 1. ã¾ãšã¯æ­£è¦åŒ–æ–‡å­—åˆ—ã§å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯ (EMBEDDINGSä¸è¦ã®ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‘ã‚¹)
                        for i, cache_item in enumerate(FAQ_CACHE):
                            if cache_item.get("norm_key") == norm_query:
                                logger.info(f"[Cache Debug] âš¡ EXACT MATCH HIT! (æ­£è¦åŒ–ã‚­ãƒ¼å®Œå…¨ä¸€è‡´)")
                                output_queue.put({"type": "debug", "msg": f"âœ… æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: æ­£è¦åŒ–ã‚­ãƒ¼å®Œå…¨ä¸€è‡´! (è³ªå•: {cache_item['question'][:15]}...)"})
                                best_idx = i
                                max_sim = 1.0
                                break
                        
                        # 2. å®Œå…¨ä¸€è‡´ã—ãªã‹ã£ãŸå ´åˆã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
                        if best_idx == -1 and EMBEDDER is not None and FAQ_EMBEDDINGS is not None and len(FAQ_EMBEDDINGS) > 0:
                            try:
                                query_embed = EMBEDDER.embed_query(item.message_text)
                                query_vector = np.array(query_embed)
                                
                                norms = np.linalg.norm(FAQ_EMBEDDINGS, axis=1) * np.linalg.norm(query_vector)
                                similarities = np.dot(FAQ_EMBEDDINGS, query_vector) / norms
                                
                                best_idx = int(np.argmax(similarities))
                                max_sim = float(similarities[best_idx])
                                logger.info(f'[Cache Debug] å…¥åŠ›: "{item.message_text}" | æœ€ã‚‚ä¼¼ã¦ã„ã‚‹FAQ: "{FAQ_CACHE[best_idx]["question"]}" | é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {max_sim:.4f}')
                                output_queue.put({"type": "debug", "msg": f"ğŸ§  æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ {max_sim:.4f} (å€™è£œ: {FAQ_CACHE[best_idx]['question'][:15]}...)"})
                            except Exception as embed_e:
                                logger.warning(f"[Worker] Embedding check failed during vector calculation: {embed_e}")
                        
                        if max_sim >= 0.75 and best_idx != -1:
                            cached_ans = FAQ_CACHE[best_idx].get("response_text", "")
                            rejection_phrases = ["ç­”ãˆã‚‰ã‚Œã¾ã›ã‚“", "å­¦ç¿’ä¸­", "ã‚¨ãƒ©ãƒ¼", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“"]
                            is_rejected = any(rp in cached_ans for rp in rejection_phrases)
                            
                            if is_rejected:
                                logger.info(f"[Worker] âš ï¸ Cache contains rejection phrase. Invalidating and flagging for auto-repair. (Idx: {best_idx})")
                                cache_to_repair = best_idx
                            else:
                                logger.info(f"[Worker] FAQ Cache HIT! Similarity: {max_sim:.2f} (Matched: {FAQ_CACHE[best_idx]['question']})")
                                logger.info("[Cache Debug] âš¡ CACHE HIT! (ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™)")
                                output_queue.put({"type": "debug", "msg": "âš¡ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: ååˆ†ãªä¸€è‡´ã€‚ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¿”ã—ã¾ã™ã€‚"})
                                best_match_item = FAQ_CACHE[best_idx]
                        else:
                            logger.info("[Cache Debug] ğŸ§  CACHE MISS. (LLMç”Ÿæˆã«é€²ã¿ã¾ã™)")
                            output_queue.put({"type": "debug", "msg": "ğŸ“ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã€‚LLMã«ã‚ˆã‚‹æ–°è¦ç”Ÿæˆã‚’æ§‹æˆä¸­..."})
                    except Exception as e:
                        logger.warning(f"[Worker] Embedding check failed: {e}")
                        output_queue.put({"type": "debug", "msg": f"âš ï¸ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªã‚¨ãƒ©ãƒ¼ ({e})ã€‚LLMç”Ÿæˆã«åˆ‡ã‚Šæ›¿ãˆã€‚"})

                if best_match_item:
                    reply_text = best_match_item["response_text"]
                    emotion = best_match_item.get("emotion", "Neutral")
                    audio_b64 = best_match_item.get("audio_b64")
                    
                    if not audio_b64:
                        logger.warning("[Worker] FAQ Cache has no audio. Generating...")
                        audio_b64 = synthesize_speech(reply_text, creds_json=creds_json, 
                                                    private_key=private_key, client_email=client_email, 
                                                    use_cache=False)
                    
                    result = {
                        "type": "result",
                        "audio_b64": audio_b64,
                        "emotion": emotion,
                        "response_text": reply_text,
                        "question": item.message_text if not is_system else "(èµ·å‹•æŒ¨æ‹¶)",
                        "author": getattr(item, "author_name", ""),
                        "is_initial_greeting": getattr(item, "is_initial_greeting", False)
                    }
                    import time
                    time.sleep(0.5) # ğŸŒŸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã€Œç·©å’Œã€ (Smoothing)
                    output_queue.put(result)
                    logger.info(f"[Worker] Task complete (FAQ Cache)")
                    continue
                
                # 2. AI Response
                output_queue.put({"type": "progress", "msg": "Thinking..."})
                reply_text, emotion = generate_response(item.message_text, api_key=google_api_key, use_cache=False)
                output_queue.put({"type": "debug", "msg": f"ğŸ¤– æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: LLMå›ç­”ç”Ÿæˆå®Œäº† ({len(reply_text)}æ–‡å­—, æ„Ÿæƒ…:{emotion})"})
                
                # 2. TTS
                output_queue.put({"type": "progress", "msg": "Synthesizing voice..."})
                output_queue.put({"type": "debug", "msg": "ğŸ¤ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: éŸ³å£°åˆæˆã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­..."})
                audio_b64 = synthesize_speech(reply_text, creds_json=creds_json, 
                                            private_key=private_key, client_email=client_email, 
                                            use_cache=False)
                
                # 3. Auto-Repair or Append Cache if needed
                is_valid_answer = True
                rejection_phrases = ["ç­”ãˆã‚‰ã‚Œã¾ã›ã‚“", "å­¦ç¿’ä¸­", "ã‚¨ãƒ©ãƒ¼", "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“"]
                if any(rp in reply_text for rp in rejection_phrases):
                    is_valid_answer = False

                if not is_system and not is_greeting and is_valid_answer:
                    if cache_to_repair is not None and FAQ_CACHE[cache_to_repair].get("source") != "master":
                        # ãƒ¬ã‚¬ã‚·ãƒ¼(ãƒã‚¹ã‚¿ãƒ¼)ä»¥å¤–ãªã‚‰ä¿®å¾©
                        logger.info(f"ğŸ”§ [Worker] Auto-repairing EXTRA cache index {cache_to_repair} with new valid answer.")
                        FAQ_CACHE[cache_to_repair]["response_text"] = reply_text
                        FAQ_CACHE[cache_to_repair]["emotion"] = emotion
                        FAQ_CACHE[cache_to_repair]["audio_b64"] = audio_b64
                    elif cache_to_repair is None:
                        logger.info(f"â• [Worker] Appending new wild question to extra cache.")
                        new_cache_entry = {
                            "question": item.message_text,
                            "response_text": reply_text,
                            "emotion": emotion,
                            "audio_b64": audio_b64,
                            "norm_key": normalize_text(item.message_text),
                            "source": "extra"
                        }
                        FAQ_CACHE.append(new_cache_entry)
                        try:
                            if EMBEDDER is not None:
                                new_embed = np.array([EMBEDDER.embed_query(item.message_text)])
                                if FAQ_EMBEDDINGS is not None:
                                    FAQ_EMBEDDINGS = np.vstack([FAQ_EMBEDDINGS, new_embed])
                                else:
                                    FAQ_EMBEDDINGS = new_embed
                        except Exception as e:
                            logger.error(f"Failed to update embeddings dynamically: {e}")
                    
                    # ğŸš€ éåŒæœŸã§æ›¸ãè¾¼ã¿ã‚’è¡Œã„ã€å¿œç­”ãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„
                    def async_write_extra_cache():
                        try:
                            # ãƒã‚¹ã‚¿ãƒ¼ä»¥å¤–ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æŠ½å‡º (source == "extra")
                            save_data = []
                            for c in FAQ_CACHE:
                                if c.get("source") == "extra":
                                    c_copy = c.copy()
                                    c_copy.pop("norm_key", None)
                                    # ãƒãƒ¼ã‚¯ã¥ã‘ã¯æ°¸ç¶šåŒ–ã™ã‚‹å¿…è¦ãªã„ã‹ã‚‚ã ãŒã€èª­ã¿è¾¼ã¿æ™‚ã«ã¤ã‘ã‚‹ã®ã§æ®‹ã—ã¦ã‚ˆã—
                                    save_data.append(c_copy)
                            
                            extra_cache_file = LOCAL_STATIC_DIR / "extra_cache.json"
                            with open(extra_cache_file, "w", encoding="utf-8") as f:
                                json.dump(save_data, f, ensure_ascii=False, indent=2)
                            logger.info(f"ğŸ’¾ [Worker] Safely saved extra_cache.json async. Extra total: {len(save_data)}")
                        except Exception as e:
                            logger.error(f"Failed to write extra cache back to disk: {e}")
                            
                    threading.Thread(target=async_write_extra_cache, daemon=True).start()

                # 4. Final Result
                result = {
                    "type": "result",
                    "audio_b64": audio_b64,
                    "emotion": emotion,
                    "response_text": reply_text,
                    "question": item.message_text if not is_system else "(èµ·å‹•æŒ¨æ‹¶)",
                    "author": getattr(item, "author_name", ""),
                    "is_initial_greeting": getattr(item, "is_initial_greeting", False)
                }
                output_queue.put(result)
                logger.info(f"[Worker] Task complete: {reply_text[:20]}...")

            except Exception as e:
                logger.error(f"[Worker] Task failed: {e}")
                output_queue.put({"type": "error", "msg": f"AI/TTS Error: {str(e)}"})
                time.sleep(2)

    logger.info("[Worker] Thread stopping.")

def init_worker():
    """Starts the background worker thread if not already running."""
    if st.session_state.worker_thread is None:
        # Get secrets once in the main thread
        # ğŸš€ Secrets ã‚’ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§å–å¾—ã—ã€Workerã¸æ˜ç¤ºçš„ã«æ¸¡ã™ (ãƒã‚±ãƒ„ãƒªãƒ¬ãƒ¼)
        api_key = st.secrets.get("FINAL_MASTER_KEY") or st.secrets.get("GOOGLE_API_KEY") or ""
        creds_json = st.secrets.get("GOOGLE_APPLICATION_CREDENTIALS_JSON") or ""
        # Individual keys
        p_key = st.secrets.get("GCP_PRIVATE_KEY") or ""
        c_email = st.secrets.get("GCP_CLIENT_EMAIL") or ""

        stop_event = threading.Event()
        thread = threading.Thread(
            target=_worker_loop,
            args=(st.session_state.queue, st.session_state.output_queue, stop_event, 
                  api_key, creds_json, p_key, c_email),
            daemon=True,
            name="avatar-worker"
        )
        thread.start()
        st.session_state.worker_thread = thread
        st.session_state.worker_stop = stop_event
        logger.info("[App] Background worker started.")
